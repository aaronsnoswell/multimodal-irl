{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Uniform Initialization Experiment\n",
    "\n",
    "Run the Babes-Vroman EM MM-IRL algorithm with uniform initialization on the deterministic Canonical PuddleWorld.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_path = %pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(notebook_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pprint\n",
    "import gym\n",
    "import copy\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from puddle_world.envs import *\n",
    "from explicit_env.soln import value_iteration, q_from_v, OptimalPolicy, policy_evaluation\n",
    "from unimodal_irl.utils import get_rollouts, empirical_feature_expectations\n",
    "\n",
    "from multimodal_irl import bv_em_maxent\n",
    "\n",
    "from unimodal_irl import sw_maxent_irl\n",
    "from unimodal_irl.utils import pad_terminal_mdp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Experimental parameters\n",
    "ENVIRONMENT = \"CanonicalPuddleWorld\"\n",
    "TRANSITIONS = 'deterministic'\n",
    "NUM_GT_CLUSTERS = 2\n",
    "NUM_CLUSTERS_SWEEP = [2, 3, 1]\n",
    "NUM_ROLLOUTS_SWEEP = [10, 50, 100]\n",
    "ALGORITHM = \"BV-MaxEnt\"\n",
    "INITIALISATION = 'kmeans'\n",
    "NUM_REPLICATES = 100\n",
    "\n",
    "if TRANSITIONS == 'deterministic':\n",
    "    wind = 0.0\n",
    "else:\n",
    "    wind = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env_wet = CanonicalPuddleWorldEnv(mode='wet', wind=wind)\n",
    "env_dry = CanonicalPuddleWorldEnv(mode='dry', wind=wind)\n",
    "env_ = copy.deepcopy(env_wet)\n",
    "\n",
    "# Pre-compute optimal SD policy value for each mode\n",
    "pi_wet = OptimalPolicy(q_from_v(value_iteration(env_wet), env_wet), stochastic=False)\n",
    "pi_wet_v = policy_evaluation(env_wet, pi_wet)\n",
    "\n",
    "pi_dry = OptimalPolicy(q_from_v(value_iteration(env_dry), env_dry), stochastic=False)\n",
    "pi_dry_v = policy_evaluation(env_dry, pi_dry)\n",
    "\n",
    "filename_wet = f\"pw-{TRANSITIONS}-wet.pkl\"\n",
    "with open(filename_wet, \"rb\") as file:\n",
    "    all_rollouts_wet = pickle.load(file)\n",
    "filename_dry = f\"pw-{TRANSITIONS}-dry.pkl\"\n",
    "with open(filename_dry, \"rb\") as file:\n",
    "    all_rollouts_dry = pickle.load(file)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    columns=[\n",
    "        # Independent Variables\n",
    "        \"Environment\",\n",
    "        \"Transition Type\",\n",
    "        \"Num GT Clusters\",\n",
    "        \"Num Learned Clusters\",\n",
    "        \"Num Rollouts\",\n",
    "        \"Algorithm\",\n",
    "        \"Initialisation\",\n",
    "        \n",
    "        \"Replicate\",\n",
    "        \n",
    "        # Dependent Variables\n",
    "        \"Iterations\",\n",
    "        \"Responsibility Matrix\",\n",
    "        \"Reward Weights\",\n",
    "        \"Runtime (s)\",\n",
    "    ],\n",
    "    index=range(len(NUM_ROLLOUTS_SWEEP) * len(NUM_CLUSTERS_SWEEP) * NUM_REPLICATES)\n",
    ")\n",
    "df[\"Responsibility Matrix\"] = df[\"Responsibility Matrix\"].astype('object')\n",
    "df[\"Reward Weights\"] = df[\"Reward Weights\"].astype('object')\n",
    "\n",
    "\n",
    "_exp_num = 0\n",
    "for num_rollouts in NUM_ROLLOUTS_SWEEP:\n",
    "    for num_clusters in NUM_CLUSTERS_SWEEP:\n",
    "        for replicate in range(NUM_REPLICATES):\n",
    "            print(f\"Exp {_exp_num}/{len(df)}, N={num_rollouts}, K={num_clusters} Replicate {replicate}/{NUM_REPLICATES}\")\n",
    "            \n",
    "            # Slice out rollouts for this experiment\n",
    "            start_idx = replicate * num_rollouts // 2\n",
    "            end_idx = (replicate + 1) * num_rollouts // 2\n",
    "            rollouts = [*all_rollouts_wet[start_idx:end_idx], *all_rollouts_dry[start_idx:end_idx]]\n",
    "            \n",
    "            rollout_features = np.array([\n",
    "                empirical_feature_expectations(env_, [r])[0]\n",
    "                for r in rollouts\n",
    "            ])\n",
    "            \n",
    "            # Run experiment\n",
    "            t0 = datetime.now()\n",
    "            \n",
    "            # Initialize mode weights with K-Means clustering\n",
    "            km = KMeans(n_clusters=num_clusters, n_init=5000)\n",
    "            hard_initial_clusters = km.fit_predict(rollout_features)\n",
    "            print(\"Initial clusters:\", hard_initial_clusters)\n",
    "            soft_initial_clusters = np.zeros((len(rollouts), num_clusters))\n",
    "            for idx, clstr in enumerate(hard_initial_clusters):\n",
    "                soft_initial_clusters[idx, clstr] = 1.0\n",
    "            \n",
    "            # Compute intial reward weights from up-front clustering\n",
    "            env_padded, rollouts_padded = pad_terminal_mdp(env_, rollouts=rollouts)\n",
    "            initial_reward_weights = []\n",
    "            for m in range(num_clusters):\n",
    "                initial_reward_weights.append(\n",
    "                    sw_maxent_irl(\n",
    "                        rollouts_padded,\n",
    "                        env_padded,\n",
    "                        rs=True,\n",
    "                        rbound=env_.reward_range,\n",
    "                        with_dummy_state=True,\n",
    "                        grad_twopoint=True,\n",
    "                        path_weights=soft_initial_clusters[:, m],\n",
    "                    )[0][:-1]\n",
    "                )\n",
    "            num_iterations, responsibility_matrix, _, reward_weights = bv_em_maxent(\n",
    "                env_,\n",
    "                rollouts,\n",
    "                num_clusters,\n",
    "                initial_reward_weights=initial_reward_weights\n",
    "            )\n",
    "            t1 = datetime.now()\n",
    "            dt = (t1 - t0).total_seconds()\n",
    "            \n",
    "            df.iloc[_exp_num] = [\n",
    "                ENVIRONMENT,\n",
    "                TRANSITIONS,\n",
    "                NUM_GT_CLUSTERS,\n",
    "                num_clusters,\n",
    "                num_rollouts,\n",
    "                ALGORITHM,\n",
    "                INITIALISATION,\n",
    "                \n",
    "                replicate,\n",
    "                \n",
    "                num_iterations,\n",
    "                responsibility_matrix.tolist(),\n",
    "                reward_weights.tolist(),\n",
    "                dt\n",
    "            ]\n",
    "            _exp_num += 1\n",
    "            \n",
    "            filename = f\"{ENVIRONMENT}-{TRANSITIONS}-{INITIALISATION}.csv\"\n",
    "            df.to_csv(filename)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
